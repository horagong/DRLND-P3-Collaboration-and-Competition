{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition Project\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1. If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01. Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Each agent receives its own, local observation. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping.\n",
    "\n",
    "The task is episodic, and in order to solve the environment, your agents must get an average score of +0.5 (over 100 consecutive episodes, after taking the maximum over both agents). Specifically,\n",
    "\n",
    "After each episode, we add up the rewards that each agent received (without discounting), to get a score for each agent. This yields 2 (potentially different) scores. We then take the maximum of these 2 scores.\n",
    "This yields a single score for each episode.\n",
    "The environment is considered solved, when the average (over 100 episodes) of those scores is at least +0.5.\n",
    "\n",
    "\n",
    "## Learning Algorithm\n",
    "\n",
    "\n",
    "The environment has two different agents. Because both of the agents perform the same task under the same conditions, I've used a signle brain to control two agents. The action space is continuous and the `value-based method` like DQN is not suitable in this environment. So I decided to use the `policy-based method`.\n",
    "\n",
    "Policy-based methods offer practical ways of dealing with large actions spaces, even continuous spaces with an infinite number of actions. Instead of computing learned probabilities for each of the many actions, we instead learn statistics of the probability distribution. \n",
    "\n",
    "I trained the network using `Deep Deterministic Policy Gradients(DDGP)` algorithm. It is an `Actor-Critic method` in which two architectures are combined. Actor determines the current policy in continuous space and Critic learns the Q-values in a given (state, action) pair. \n",
    "The Actor and Critic network has three layers with 400 and 300 units in hidden layers and the 8 dimensional vector is used as the input. The final layer of Actor has 4 actions and the final layer of Critic is fully connected with the size of 1.\n",
    "\n",
    "DDPG Hyperparameters:\n",
    "```python\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 128        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-4         # learning rate of the actor \n",
    "LR_CRITIC = 1e-4        # learning rate of the critic\n",
    "WEIGHT_DECAY = 1e-5        # L2 weight decay\n",
    "```\n",
    "## Plot of Rewards\n",
    "\n",
    "The agents get an average score of +0.5 (over 100 consecutive episodes, after taking the maximum over both agents).\n",
    "![rewards](rewards.png)\n",
    "\n",
    "## Ideas for Future Work\n",
    "* add batch normalization to actor networks\n",
    "* try a different learning algorithm like D4PG, PPO or A3C.\n",
    "* immplement Prioritized Experience Replay.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
